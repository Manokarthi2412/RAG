# RAG System

This system utilizes a Retrieval-Augmented Generation (RAG)  to process various file types and generate contextually accurate responses. This system supports text, video transcripts, CSV files, PDF documents, and image files. It uses embeddings and vector databases to enhance query responses.


- **Model for Embedding Creation**: all-MiniLM-L6-v2
- **Model for Response Generation**: gemini-1.5-flash

## Workflow

### 1. Embedding Creation
- Text from source files (e.g., PDFs, Word Docs, CSVs) is converted into vector embeddings using the `all-MiniLM-L6-v2` model. These embeddings are stored in a vector database for fast similarity search.

### 2. Query Processing
- User queries are embedded using the same model, and relevant context is retrieved through a similarity search.

### 3. Response Generation
- A prompt is generated by combining the user query and the retrieved context. The `gemini-1.5-flash` model generates a contextually accurate response.

## Implementation Details

### Data Preprocessing

## PDF Processing

- **Text Extraction**: Text is extracted from PDF files on a page-by-page basis using the `PyMuPDF` library.
- **Image Extraction**: Images embedded within PDFs are also extracted using `PyMuPDF`.

## FAQs Extraction

- Relevant FAQ data is scraped from Stack Overflow threads, extracting questions along with only the accepted answers.
- **Libraries Used**: `Selenium`, `BeautifulSoup`

## Video Processing

- Video content is converted into audio using the `MoviePy` library.

## Audio Transcription

- The extracted audio is transcribed into text using the `SpeechRecognition` library, which leverages Google Speech Recognition.


### Embedding Generation
- **Model**: all-MiniLM-L6-v2
- **Reason**: This model maps sentences into a dense vector space, making it ideal for clustering and semantic search tasks.

### Vector Indexing
- **Database**: FAISS
- **Index Type**: Flat indexing for exhaustive search, ensuring high accuracy.

### RAG Workflow

1. Source files are embedded and stored in the FAISS vector database.
2. User queries are embedded and matched against stored embeddings to retrieve relevant context.
3. A response is generated using `gemini-1.5-flash`.

## Evaluation

- **Retrieval Evaluation**: Evaluated using keyword matching.
- **Generation Evaluation**: Evaluated by calculating cosine similarity between actual and generated responses.


## File Overview

## File Overview

- **constants.py**: Defines and initializes the LLM, embedding models, and vector database.
- **vector_store.py**: Contains functions for data preprocessing, embedding generation, and vector database storage (FAISS).
- **model.py**: Implements context retrieval and response generation using the LLM.
- **app.py**: Contains Flask code to run the application.


## Commands to Run the System

```requirements
#faiss vectorstore 
pip install faiss-cpu

# PDF Processing
pip install pymupdf

# FAQ Extraction
pip install selenium
pip install beautifulsoup4

# Video Processing
pip install moviepy

# Audio Transcription
pip install SpeechRecognition

```

### 1. Navigate to the Project Directory

```bash
cd <project-directory>

```

```
python app.py
```
